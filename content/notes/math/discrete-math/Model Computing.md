---
title: Model Computing
enableToc: true
tags: 
- model-computing
---

## Phrase-Structure Grammars

- A ***vocabulary*** (or *alphabet*, *азбука*) $V$ is a finite, nonempty [[notes/math/discrete-math/set-theory/Set|set]] of elements called ***symbols*** (*букви*)
- А ***word*** (or *sentence*, *дума*) over $V$ is a string of finite length of elements of $V$
- The ***empty string*** (or *null string*, *празна дума*), denoted by $\lambda$ (or $\varepsilon$), is the string containing no symbols.
- The set of all words over $V$ is denoted by $V^*$ 
- A ***language*** (or *език*) over $V$ is a subset of $V^*$.

> Note that $\lambda$, the empty string, is the string containing no symbols. It is different from $\varnothing$, the empty set. It follows that $\lbrace \lambda  \rbrace$ is the set containing exactly one string, namely, the empty string.

A **grammar** (*граматика*) provides a set of symbols of various types and a set of rules for producing words. More precisely, a grammar has a **vocabulary** $V$ , which is a set of symbols used to derive members of the language. Some of the elements of the $V$ cannot be replaced by other symbols. These are called **terminals** (*терминали*), denoted by $T$, and the other members of the $V$, which can be replaced by other symbols, are called **nonterminals** (*нетерминали*), denoted by $N$. There is a special member of the vocabulary called the **start symbol** (*аксиома, начален терминал*), denoted by $S$, which is the element of the vocabulary that we always begin with. The rules that specify when we can replace a string from $V^*$, the set of all strings of elements in the vocabulary, with another string are called the **productions** of the grammar. We denote by $z_0 → z_1$ the production that specifies that $z_0$ can be replaced by $z_1$ within a string.

A *phrase-structure grammar* $G = (V, T, S, P)$ consists of **vocabulary** $V$, a subset $T$ of $V$ consisting of **terminal symbols**, a **start symbol** $S$ from $V$, and a finite set of productions $P$. The set $V - T$ is denoted by $N$. Elements of $N$ are called **nonterminal symbols**. Every production in $P$ must contain at least one nonterminal on its leftside.

**Example 1.** Let $G = (V, T, S, P)$, where $V =  \lbrace a, b, A, B, S \rbrace, T =  \lbrace a,b \rbrace$, $S$ is the start symbol, and $P =  \lbrace S \rightarrow ABa, A \rightarrow BB, B \rightarrow ab, AB \rightarrow b \rbrace$. $G$ is an example of phrase-structure grammar.

Let $G = (V, T, S, P)$ be a phrase-structure grammar. Let $w_0 = lz_0r$ (that is, the concatenation of $l$, $z_0$, and $r$) and $w_1 = lz_1r$ be strings over $V$. If $z_0 \rightarrow z_1$ is production of $G$, we say that $w_1$ is *directly derivable* from $w_0$ and we write $w_0 \implies w_1$. If $w_0, w_1, ..., w_n$ are strings over $V$ such that $w_0 \implies w_1, w_1 \implies w_2, ..., w_{n-1} \implies w_n$, then we say that $w_n$ is *derivable* from $w_0$, and we write $w_0 \overset{*}{ \implies } w_n$. The sequence of steps used to obtain $w_n$ from $w_0$ is called *a derivation*.

**Example 2.** The string `Aaba` is directly derivable from `ABa` in the grammar in **Example 1** because $B \rightarrow ab$ is a production in the grammar. The string `abababa` is derivable from `ABa` because $AB \implies Aaba \implies BBaba \implies Bababa \implies abababa$, using the productions $B \rightarrow ab, A \rightarrow BB, B \rightarrow ab$ and $B \rightarrow ab$ in succession.

Let $G = (V, T, S, P)$ by a phrase-structure grammar. The *language generated by* $G$ (*or the language of* $G$), denoted by $L(G)$, is the set of all strings of terminals that are derivable from starting state $S$. In other words,
$$
L(G) =  \lbrace w \in T^{*}\ |\ S \overset{*}{ \implies } w \rbrace
$$
**Example 3.** Let $G$ be the grammar with vocabulary $V =  \lbrace S, A, a, b \rbrace$, set of terminals $T =  \lbrace a, b \rbrace$, starting symbol $S$, and production $P =  \lbrace S \rightarrow aA, S \rightarrow b, A \rightarrow aa \rbrace$. What is $L(G)$, the language of this grammar?

*Solution*: From the start state $S$ we can derive `aA` using the production $S \rightarrow aA$. We can also use the production $S \rightarrow b$ to derive $b$. From $aA$ the production $A \rightarrow aa$ can be used to derive `aaa`. No additional words can be derived. Hence, $L(G) =  \lbrace b, aaa \rbrace$.

### Types of Phrase-Structure Grammars

- **Type 0**: has no restrictions on its productions
- **Type 1**: can have productions of the form $w_1 → w_2$, where $w_1 = lAr$ and $w_2 = lwr$, where $A$ is a nonterminal symbol, $l$ and $r$ are strings of zero or more terminal or nonterminal symbols, and $w$ is a nonempty string of terminal or nonterminal symbols. It can also have the production $S → λ$ as long as $S$ does not appear on the right-hand side of any other production (**context-sensitive grammars**)
- **Type 2**: can have productions only of the form $w_1 → w_2$, where $w_1$ is a single symbol that is not a terminal symbol (**context-free grammars**)
- **Type 3**: can have productions only of the form $w_1 → w_2$ with $w_1 = A$ and either $w_2 = aB$ or $w_2 = a$, where $A$ and $B$ are nonterminal symbols and a is a terminal symbol, or with $w_1 =S$ and $w_2 = λ$. (**regular grammars**)

### Derivation Trees

A derivation in the language generated by a context-free grammar can be represented graphically using an ordered rooted tree, called a derivation, or parse tree. The root of this tree represents the starting symbol. The internal vertices of the tree represent the nonterminal symbols that arise in the derivation. The leaves of the tree represent the terminal symbols that arise. If the production $A →$ w arises in the derivation, where $w$ is a word, the vertex that represents A has as children vertices that represent each symbol in w, in order from left to right.

![adjacency graph](notes/assets/parse-tree.png#invert_B)
